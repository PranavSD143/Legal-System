{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70343d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "model = OllamaLLM(model=\"openchat:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eeaccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_components(query):\n",
    "  \n",
    "  prompt = f\"\"\"\n",
    "You are a legal information extractor AI specializing in the Constitution of India.\n",
    "\n",
    "Your task is to process a user's natural language query and return ONLY a valid JSON object with the following structure:\n",
    "\n",
    "1. \"tags\": A list of 8 to 10 **semantically relevant legal concepts**. These must:\n",
    "   - Be meaningful legal, constitutional, civic, or administrative concepts.\n",
    "   - Be loosely or indirectly inferable from the query even if not explicitly stated.\n",
    "   - Be written in lowercase.\n",
    "   - **MUST NOT include article numbers or part names** (e.g., \"article 243A\", \"part IXA\").\n",
    "\n",
    "2. \"article_nums\": A **list of article numbers** explicitly mentioned in the query (e.g., [\"14\", \"15\"]). If none are found, return an empty list.\n",
    "\n",
    "3. \"part\": The **exact constitutional part name** mentioned (e.g., \"Part III\"). If none is found, return null.\n",
    "\n",
    "You MUST return **only the JSON object**. No extra text, formatting, or keys.\n",
    "\n",
    "### Output Format:\n",
    "{{\n",
    "  \"tags\": [\"tag1\", \"tag2\", \"... up to 10 tags ...\"],\n",
    "  \"article_nums\": [\"14\", \"15\"],\n",
    "  \"part\": \"Part III\"\n",
    "}}\n",
    "\n",
    "Now extract the data from this query:\n",
    "\"{query}\"\n",
    "\"\"\"\n",
    "\n",
    "  response = model.invoke(prompt)\n",
    "  return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_articles(user_query, tag_index, base_threshold=0.6):\n",
    "    # Extract structured components from the query\n",
    "    query_data = json.loads(extract_query_components(user_query))\n",
    "    query_tags = query_data['tags']\n",
    "    part = query_data['part']\n",
    "    article_nums = query_data['article_nums']\n",
    "\n",
    "    PART_MATCH_WEIGHT = 1.2\n",
    "    ARTICLE_MATCH_WEIGHT = 1.4\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Handle empty or bad tag extraction\n",
    "    if not query_tags:\n",
    "        return []\n",
    "\n",
    "    # Dynamic threshold adjustment based on tag count\n",
    "    tag_count = len(query_tags)\n",
    "    adjusted_threshold = base_threshold - (0.05 * max(0, 10 - tag_count))\n",
    "\n",
    "    # Embed user query tags\n",
    "    query_vector = embed.embed_query(\" \".join(query_tags))\n",
    "    qv = np.array(query_vector).reshape(1, -1)\n",
    "    \n",
    "    for tag_vector, article in tag_index:\n",
    "        tv = np.array(tag_vector).reshape(1, -1)\n",
    "\n",
    "        try:\n",
    "            similarity = cosine_similarity(qv, tv)[0][0]\n",
    "        except Exception as e:\n",
    "            print(\"Similarity calculation failed:\", e)\n",
    "            continue\n",
    "\n",
    "        # Optional: compute tag overlap score\n",
    "        article_tags = article.get(\"tags\", [])\n",
    "        tag_overlap = len(set(query_tags) & set(article_tags)) / max(len(set(query_tags)), 1)\n",
    "\n",
    "        # Combine both similarity + tag overlap\n",
    "        combined_score = 0.7 * similarity + 0.3 * tag_overlap\n",
    "\n",
    "        # Boost for part and article number matches\n",
    "        if part and article[\"metadata\"].get(\"part\") == part:\n",
    "            combined_score *= PART_MATCH_WEIGHT\n",
    "        if article_nums and article[\"metadata\"].get(\"article\") == article_nums:\n",
    "            combined_score *= ARTICLE_MATCH_WEIGHT\n",
    "\n",
    "        if combined_score >= adjusted_threshold:\n",
    "            results.append((combined_score, article))\n",
    "\n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6433b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What are the rights and duties mentioned in Article 243A of Part IXA?\"\n",
    "\n",
    "# tag_index = load_tag_index()\n",
    "# match = filter_articles(query, tag_index)\n",
    "\n",
    "# for score, article in match:\n",
    "#     article_num = article[\"metadata\"].get(\"article\", \"Unknown\")\n",
    "#     part = article[\"metadata\"].get(\"part\", \"Unknown\")\n",
    "#     preview = article.get(\"text\", \"\")[:120].strip() + \"...\"\n",
    "    \n",
    "#     print(f\"Score: {score:.2f} | Article: {article_num} | Part: {part}\")\n",
    "#     print(f\"Text: {preview}\")\n",
    "#     print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
