{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70343d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26068f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles(file_path = 'articles.json'):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcc8a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing \n",
    "def extract_query_components(query):\n",
    "    # Normalize query\n",
    "    query = query.lower().strip()\n",
    "    query = re.sub(r\"[^\\w\\s]\", \"\", query)  # remove punctuation\n",
    "\n",
    "    words = query.split()\n",
    "    part = None\n",
    "    article_num = None\n",
    "    tags = []\n",
    "\n",
    "    # Capture article number if present\n",
    "    for i, word in enumerate(words):\n",
    "        if word == \"article\" and i + 1 < len(words):\n",
    "            article_num = words[i + 1].upper()\n",
    "\n",
    "        elif word == \"part\" and i + 1 < len(words):\n",
    "            part_name = words[i + 1].upper()\n",
    "            if not part_name.startswith(\"PART\"):\n",
    "                part = \"Part \" + part_name\n",
    "            else:\n",
    "                part = part_name\n",
    "\n",
    "    # Basic stopword filtering\n",
    "    stopwords = {\"what\", \"are\", \"is\", \"in\", \"of\", \"the\", \"mentioned\", \"and\", \"to\", \"a\", \"an\", \"on\", \"under\", \"with\"}\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stopwords and word != \"article\" and word != \"part\":\n",
    "            # Filter out numeric-like strings and previously extracted elements\n",
    "            if word.upper() != article_num and not word.upper().startswith(\"PART\"):\n",
    "                tags.append(word)\n",
    "\n",
    "    return {\n",
    "        \"tags\": tags,\n",
    "        \"part\": part,\n",
    "        \"article_num\": article_num\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1182a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tag_embeddings(articles):\n",
    "    tag_embeddings = []\n",
    "    for art in articles:\n",
    "        tag_string = \" \".join(art['metadata'].get(\"tags\", []))\n",
    "        if tag_string.strip():\n",
    "            embedding = embed.embed_query(tag_string)\n",
    "            tag_embeddings.append((embedding, art))\n",
    "    return tag_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def filter_articles(user_query, tag_index, threshold=0.6):\n",
    "    # Extract structured components from the query\n",
    "    query_data = extract_query_components(user_query)\n",
    "    query_tags = query_data['tags']\n",
    "    part = query_data['part']\n",
    "    article_num = query_data['article_num']\n",
    "\n",
    "    PART_MATCH_WEIGHT = 1.2\n",
    "    ARTICLE_MATCH_WEIGHT = 1.4\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Embed user query tags\n",
    "    query_vector = embed.embed_query(\" \".join(query_tags))\n",
    "    qv = np.array(query_vector).reshape(1, -1)\n",
    "    \n",
    "    for tag_vector, article in tag_index:\n",
    "        # ðŸ§ª Debug: Check types and shapes\n",
    "        tv = np.array(tag_vector).reshape(1, -1)\n",
    "\n",
    "        # Try converting to lists if needed\n",
    "        try:\n",
    "            similarity = cosine_similarity(qv, tv)[0][0]\n",
    "        except Exception as e:\n",
    "            print(\"Similarity calculation failed:\", e)\n",
    "            continue\n",
    "\n",
    "        score = similarity\n",
    "        # Boost score for part and article_num match\n",
    "        if part and part == article[\"metadata\"].get(\"part\"):\n",
    "            score *= PART_MATCH_WEIGHT\n",
    "        if article_num and article_num == article[\"metadata\"].get(\"article\"):\n",
    "            score *= ARTICLE_MATCH_WEIGHT\n",
    "\n",
    "        if score >= threshold:\n",
    "            results.append((score, article))\n",
    "\n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ddf9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_articles(results, top_n=5):\n",
    "    for i, (score, art) in enumerate(results[:top_n]):\n",
    "        print(f\"\\nRank {i+1} | Score: {score:.2f}\")\n",
    "        print(f\"Article {art['metadata'].get('article')} | Part: {art['metadata'].get('part')}\")\n",
    "        print(\"Text:\", art['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c31ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def load_tag_index(path=\"tag_index.pkl\"):\n",
    "    with open(path, \"rb\") as f:\n",
    "        tag_index = pickle.load(f)\n",
    "    print(f\"Tag index loaded from {path}\")\n",
    "    return tag_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6433b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the rights and duties mentioned in Article 243A of Part IXA?\"\n",
    "\n",
    "tag_index = load_tag_index()\n",
    "match = filter_articles(query, tag_index, threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# # run once\n",
    "# def save_tag_index(tag_index, path=\"tag_index.pkl\"):\n",
    "#     with open(path, \"wb\") as f:\n",
    "#         pickle.dump(tag_index, f)\n",
    "#     print(f\"Tag index saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c40003ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag index saved to tag_index.pkl\n"
     ]
    }
   ],
   "source": [
    "save_tag_index(tag_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
